* fork()
  Unix/Linux 操作系统提供了一个 fork()系统调用，它非常特殊。
  普通的函数调用，调用一次，返回一次，但是 fork()调用一次，返回两次，因为操作系统自动把当前进程（称为父进程）复制了一份（称为子进程），然后，分别在父进程和子进程内返回。

  子进程永远返回 0，而父进程返回子进程的 ID。
  这样做的理由是，一个父进程可以 fork 出很多子进程，所以，父进程要记下每个子进程的 ID，而子进程只需要调用 getppid()就可以拿到父进程的 ID。

* 文件描述符(File descriptor)
  文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表, 该表称为描述符表(descriptor table)。
  当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。
  在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。
  但是文件描述符这一概念往往只适用于 UNIX、Linux 这样的操作系统。

* inode(索引节点)
** 简介
  文件储存在硬盘上, 硬盘的最小存储单位叫做扇区(Sector, 0.5KB), 多个扇区组成块(Block, 一般为 4k).
  操作系统以块为单由于存储介质的特性，磁盘本身存取就比主存慢很多，再加上机械运动耗费，磁盘的存取速度往往是主存的几百分分之一，因此为了提高效率，要尽量减少磁盘 I/O。为了达到这个目的，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。这样做的理论依据是计算机科学中著名的局部性原理：

位读取硬盘.

  一个文件使用一个 inode(索引节点), inode 中记录文件的元信息. 
  inode 的数目和本身的大小在格式化时指定, 因此文件系统所能建立的文件数目是有限制的.
  
  inode 包含:
  + inode 编号
  + 文件类型
  + 文件的字节数
  + 拥有者的 Group ID
  + 文件的 Group ID
  + 读写权限
  + 时间戳, 一个三个
    + ctime, 指 inode 上一次变动的时间.
    + mtime, 指文件内容上一次变动的时间.
    + atime, 指文件上一次打开的时间.
    + 链接数, 有多少个文件名指向这个 inode(硬链接的数量)
  + 文件 block 的位置.

  用户通过文件名，打开文件。实际上，系统内部这个过程分成三步：
  1. 首先，系统找到这个文件名对应的 inode 号码；
  2. 其次，通过 inode 号码，获取 inode 信息；
  3. 最后，根据 inode 信息，找到文件数据所在的 block，读出数据。

  文件名与 inode 的映射关系实际上是保存在目录项中，目录（directory）也是一种文件，
  文件内容是该目录下的文件名以及该文件名对应的 inode 号码。

** 硬链接和软链接
   硬链接源文件与目标文件的 inode 号码相同，都指向同一个 inode。inode 信息中有一项叫做”链接数”，记录指向该 inode 的文件名总数，这时就会增加 1。
   删除一个文件名，就会使得 inode 节点中的”链接数”减 1。当这个值减到 0，表明没有文件名指向这个 inode，系统就会回收这个 inode 号码，以及其所对应 block 区域。 
   软链接源文件的内容是目标文件的路径。读取软连接文件时，系统会自动将访问者导向目标文件。如果删除了目标文件，打开软连接文件时就会报错：”No such file or directory”。



* linux 进程数据结构: task_struct

#+BEGIN_SRC c

  struct task_struct
  {
    /*
      1. state: 进程执行时，它会根据具体情况改变状态。进程状态是进程调度和对换的依据。Linux 中的进程主要有如下状态:
      1) TASK_RUNNING: 可运行
      处于这种状态的进程，只有两种状态:
      1.1) 正在运行
      正在运行的进程就是当前进程(由 current 所指向的进程)
      1.2) 正准备运行
      准备运行的进程只要得到 CPU 就可以立即投入运行，CPU 是这些进程唯一等待的系统资源，系统中有一个运行队列(run_queue)，
      用来容纳所有处于可运行状态的进程，调度程序执行时，从中选择一个进程投入运行

      2) TASK_INTERRUPTIBLE: 可中断的等待状态，是针对等待某事件或其他资源的睡眠进程设置的，
      在内核发送信号给该进程表明事件已经发生时，进程状态变为 TASK_RUNNING，它只要调度器选中该进程即可恢复执行

      3) TASK_UNINTERRUPTIBLE: 不可中断的等待状态
      处于该状态的进程正在等待某个事件(event)或某个资源，它肯定位于系统中的某个等待队列(wait_queue)中，
      处于不可中断等待态的进程是因为硬件环境不能满足而等待，例如等待特定的系统资源，它任何情况下都不能被打断，
      只能用特定的方式来唤醒它，例如唤醒函数 wake_up()等,它们不能由外部信号唤醒，只能由内核亲自唤醒

      4) TASK_ZOMBIE: 僵死
      进程虽然已经终止，但由于某种原因，父进程还没有执行 wait()系统调用，终止进程的信息也还没有回收。
      顾名思义，处于该状态的进程就是死进程，这种进程实际上是系统中的垃圾，必须进行相应处理以释放其占用的资源。

      5) TASK_STOPPED: 暂停
      此时的进程暂时停止运行来接受某种特殊处理。通常当进程接收到 SIGSTOP、SIGTSTP、SIGTTIN 或 SIGTTOU 信号后就处于这种状态。
      例如，正接受调试的进程就处于这种状态
      　　　　
      　　　　　6) TASK_TRACED
      　　　　　从本质上来说，这属于 TASK_STOPPED 状态，用于从停止的进程中，将当前被调试的进程与常规的进程区分开来
      　　　　　　
      　　　　　7) TASK_DEAD
      　　　　　父进程 wait 系统调用发出后，当子进程退出时，父进程负责回收子进程的全部资源，子进程进入 TASK_DEAD 状态

      8) TASK_SWAPPING: 换入/换出
    ,*/
    volatile long state;

    /*
      2. stack
      进程内核栈，进程通过 alloc_thread_info 函数分配它的内核栈，通过 free_thread_info 函数释放所分配的内核栈
    ,*/
    void *stack;

    /*
      3. usage
      进程描述符使用计数，被置为 2 时，表示进程描述符正在被使用而且其相应的进程处于活动状态
      */
      atomic_t usage;

    /*
      4. flags
      flags 是进程当前的状态标志(注意和运行状态区分)
      1) #define PF_ALIGNWARN    0x00000001: 显示内存地址未对齐警告
      2) #define PF_PTRACED    0x00000010: 标识是否是否调用了 ptrace
      3) #define PF_TRACESYS    0x00000020: 跟踪系统调用
      4) #define PF_FORKNOEXEC 0x00000040: 已经完成 fork，但还没有调用 exec
      5) #define PF_SUPERPRIV    0x00000100: 使用超级用户(root)权限
      6) #define PF_DUMPCORE    0x00000200: dumped core
      7) #define PF_SIGNALED    0x00000400: 此进程由于其他进程发送相关信号而被杀死
      8) #define PF_STARTING    0x00000002: 当前进程正在被创建
      9) #define PF_EXITING    0x00000004: 当前进程正在关闭
      10) #define PF_USEDFPU    0x00100000: Process used the FPU this quantum(SMP only)
      #define PF_DTRACE    0x00200000: delayed trace (used on m68k)
    ,*/
    unsigned int flags;

    /*
      5. ptrace
      ptrace 系统调用，成员 ptrace 被设置为 0 时表示不需要被跟踪，它的可能取值如下：
      linux-2.6.38.8/include/linux/ptrace.h
      1) #define PT_PTRACED    0x00000001
      2) #define PT_DTRACE    0x00000002: delayed trace (used on m68k, i386)
      3) #define PT_TRACESYSGOOD    0x00000004
      4) #define PT_PTRACE_CAP    0x00000008: ptracer can follow suid-exec
      5) #define PT_TRACE_FORK    0x00000010
      6) #define PT_TRACE_VFORK    0x00000020
      7) #define PT_TRACE_CLONE    0x00000040
      8) #define PT_TRACE_EXEC    0x00000080
      9) #define PT_TRACE_VFORK_DONE    0x00000100
      10) #define PT_TRACE_EXIT    0x00000200
    ,*/
    unsigned int ptrace;
    unsigned long ptrace_message;
    siginfo_t *last_siginfo;

    /*
      6. lock_depth
      用于表示获取大内核锁的次数，如果进程未获得过锁，则置为-1
    ,*/
    int lock_depth;

    /*
      7. oncpu
      在 SMP 上帮助实现无加锁的进程切换(unlocked context switches)
    ,*/
  #ifdef CONFIG_SMP
  #ifdef __ARCH_WANT_UNLOCKED_CTXSW
    int oncpu;
  #endif
  #endif

    /*
      8. 进程调度
      1) prio: 调度器考虑的优先级保存在 prio，由于在某些情况下内核需要暂时提高进程的优先级，因此需要第三个成员来表示(除了 static_prio、normal_prio 之外)，由于这些改变不是持久的，因此静态(static_prio)和普通(normal_prio)优先级不受影响
      2) static_prio: 用于保存进程的"静态优先级"，静态优先级是进程"启动"时分配的优先级，它可以用 nice、sched_setscheduler 系统调用修改，否则在进程运行期间会一直保持恒定
      3) normal_prio: 表示基于进程的"静态优先级"和"调度策略"计算出的优先级，因此，即使普通进程和实时进程具有相同的静态优先级(static_prio)，其普通优先级(normal_prio)也是不同的。进程分支时(fork)，新创建的子进程会集成普通优先级
    ,*/
    int prio, static_prio, normal_prio;
    /*
      4) rt_priority: 表示实时进程的优先级，需要明白的是，"实时进程优先级"和"普通进程优先级"有两个独立的范畴，实时进程即使是最低优先级也高于普通进程，最低的实时优先级为 0，最高的优先级为 99，值越大，表明优先级越高
    ,*/
    unsigned int rt_priority;
    /*
      5) sched_class: 该进程所属的调度类，目前内核中有实现以下四种：
      5.1) static const struct sched_class fair_sched_class;
      5.2) static const struct sched_class rt_sched_class;
      5.3) static const struct sched_class idle_sched_class;
      5.4) static const struct sched_class stop_sched_class;
    ,*/
    const struct sched_class *sched_class;
    /*
      6) se: 用于普通进程的调用实体
      　　调度器不限于调度进程，还可以处理更大的实体，这可以实现"组调度"，可用的 CPU 时间可以首先在一般的进程组(例如所有进程可以按所有者分组)之间分配，接下来分配的时间在组内再次分配
      　　这种一般性要求调度器不直接操作进程，而是处理"可调度实体"，一个实体有 sched_entity 的一个实例标识
      　　在最简单的情况下，调度在各个进程上执行，由于调度器设计为处理可调度的实体，在调度器看来各个进程也必须也像这样的实体，因此 se 在 task_struct 中内嵌了一个 sched_entity 实例，调度器可据此操作各个 task_struct
    ,*/
    struct sched_entity se;
    /*
      7) rt: 用于实时进程的调用实体
    ,*/
    struct sched_rt_entity rt;

  #ifdef CONFIG_PREEMPT_NOTIFIERS
    /*
      9. preempt_notifier
      preempt_notifiers 结构体链表
    ,*/
    struct hlist_head preempt_notifiers;
  #endif

    /*
      10. fpu_counter
      FPU 使用计数
    ,*/
    unsigned char fpu_counter;

  #ifdef CONFIG_BLK_DEV_IO_TRACE
    /*
      11. btrace_seq
      blktrace 是一个针对 Linux 内核中块设备 I/O 层的跟踪工具
    ,*/
    unsigned int btrace_seq;
  #endif

    /*
      12. policy
      policy 表示进程的调度策略，目前主要有以下五种：
      1) #define SCHED_NORMAL        0: 用于普通进程，它们通过完全公平调度器来处理
      2) #define SCHED_FIFO        1: 先来先服务调度，由实时调度类处理
      3) #define SCHED_RR            2: 时间片轮转调度，由实时调度类处理
      4) #define SCHED_BATCH        3: 用于非交互、CPU 使用密集的批处理进程，通过完全公平调度器来处理，调度决策对此类进程给与"冷处理"，它们绝不会抢占 CFS 调度器处理的另一个进程，因此不会干扰交互式进程，如果不打算用 nice 降低进程的静态优先级，同时又不希望该进程影响系统的交互性，最适合用该调度策略
      5) #define SCHED_IDLE        5: 可用于次要的进程，其相对权重总是最小的，也通过完全公平调度器来处理。要注意的是，SCHED_IDLE 不负责调度空闲进程，空闲进程由内核提供单独的机制来处理
      只有 root 用户能通过 sched_setscheduler()系统调用来改变调度策略
    ,*/
    unsigned int policy;

    /*
      13. cpus_allowed
      cpus_allowed 是一个位域，在多处理器系统上使用，用于控制进程可以在哪里处理器上运行
    ,*/
    cpumask_t cpus_allowed;

    /*
      14. RCU 同步原语
    ,*/
  #ifdef CONFIG_TREE_PREEMPT_RCU
    int rcu_read_lock_nesting;
    char rcu_read_unlock_special;
    struct rcu_node *rcu_blocked_node;
    struct list_head rcu_node_entry;
  #endif /* #ifdef CONFIG_TREE_PREEMPT_RCU */

  #if defined(CONFIG_SCHEDSTATS) || defined(CONFIG_TASK_DELAY_ACCT)
    /*
      15. sched_info
      用于调度器统计进程的运行信息
    ,*/
    struct sched_info sched_info;
  #endif

    /*
      16. tasks
      通过 list_head 将当前进程的 task_struct 串联进内核的进程列表中，构建；linux 进程链表
    ,*/
    struct list_head tasks;

    /*
      17. pushable_tasks
      limit pushing to one attempt
    ,*/
    struct plist_node pushable_tasks;

    /*
      18. 进程地址空间
      1) mm: 指向进程所拥有的内存描述符
      2) active_mm: active_mm 指向进程运行时所使用的内存描述符
      对于普通进程而言，这两个指针变量的值相同。但是，内核线程不拥有任何内存描述符，所以它们的 mm 成员总是为 NULL。当内核线程得以运行时，它的 active_mm 成员被初始化为前一个运行进程的 active_mm 值
    ,*/
    struct mm_struct *mm, *active_mm;

    /*
      19. exit_state
      进程退出状态码
    ,*/
    int exit_state;

    /*
      20. 判断标志
      1) exit_code
      exit_code 用于设置进程的终止代号，这个值要么是_exit()或 exit_group()系统调用参数(正常终止)，
      要么是由内核提供的一个错误代号(异常终止)
      2) exit_signal
      exit_signal 被置为-1 时表示是某个线程组中的一员。只有当线程组的最后一个成员终止时，才会产生一个信号，
      以通知线程组的领头进程的父进程
    ,*/
    int exit_code, exit_signal;
    /*
      3) pdeath_signal
      pdeath_signal 用于判断父进程终止时发送信号
    ,*/
    int pdeath_signal;
    /*
      4)  personality 用于处理不同的 ABI，它的可能取值如下：
      enum
      {
      PER_LINUX =        0x0000,
      PER_LINUX_32BIT =    0x0000 | ADDR_LIMIT_32BIT,
      PER_LINUX_FDPIC =    0x0000 | FDPIC_FUNCPTRS,
      PER_SVR4 =        0x0001 | STICKY_TIMEOUTS | MMAP_PAGE_ZERO,
      PER_SVR3 =        0x0002 | STICKY_TIMEOUTS | SHORT_INODE,
      PER_SCOSVR3 =        0x0003 | STICKY_TIMEOUTS |
      WHOLE_SECONDS | SHORT_INODE,
      PER_OSR5 =        0x0003 | STICKY_TIMEOUTS | WHOLE_SECONDS,
      PER_WYSEV386 =        0x0004 | STICKY_TIMEOUTS | SHORT_INODE,
      PER_ISCR4 =        0x0005 | STICKY_TIMEOUTS,
      PER_BSD =        0x0006,
      PER_SUNOS =        0x0006 | STICKY_TIMEOUTS,
      PER_XENIX =        0x0007 | STICKY_TIMEOUTS | SHORT_INODE,
      PER_LINUX32 =        0x0008,
      PER_LINUX32_3GB =    0x0008 | ADDR_LIMIT_3GB,
      PER_IRIX32 =        0x0009 | STICKY_TIMEOUTS,
      PER_IRIXN32 =        0x000a | STICKY_TIMEOUTS,
      PER_IRIX64 =        0x000b | STICKY_TIMEOUTS,
      PER_RISCOS =        0x000c,
      PER_SOLARIS =        0x000d | STICKY_TIMEOUTS,
      PER_UW7 =        0x000e | STICKY_TIMEOUTS | MMAP_PAGE_ZERO,
      PER_OSF4 =        0x000f,
      PER_HPUX =        0x0010,
      PER_MASK =        0x00ff,
      };
    ,*/
    unsigned int personality;
    /*
      5) did_exec
      did_exec 用于记录进程代码是否被 execve()函数所执行
    ,*/
    unsigned did_exec:1;
    /*
      6) in_execve
      in_execve 用于通知 LSM 是否被 do_execve()函数所调用
    ,*/
    unsigned in_execve:1;
    /*
      7) in_iowait
      in_iowait 用于判断是否进行 iowait 计数
    ,*/
    unsigned in_iowait:1;

    /*
      8) sched_reset_on_fork
      sched_reset_on_fork 用于判断是否恢复默认的优先级或调度策略
    ,*/
    unsigned sched_reset_on_fork:1;

    /*
      21. 进程标识符(PID)
      在 CONFIG_BASE_SMALL 配置为 0 的情况下，PID 的取值范围是 0 到 32767，即系统中的进程数最大为 32768 个
      #define PID_MAX_DEFAULT (CONFIG_BASE_SMALL ? 0x1000 : 0x8000)
      在 Linux 系统中，一个线程组中的所有线程使用和该线程组的领头线程(该组中的第一个轻量级进程)相同的 PID，并被存放在 tgid 成员中。
      只有线程组的领头线程的 pid 成员才会被设置为与 tgid 相同的值。注意，getpid()系统调用
      返回的是当前进程的 tgid 值而不是 pid 值。
    ,*/
    pid_t pid;
    pid_t tgid;

  #ifdef CONFIG_CC_STACKPROTECTOR
    /*
      22. stack_canary
      防止内核堆栈溢出，在 GCC 编译内核时，需要加上-fstack-protector 选项
    ,*/
    unsigned long stack_canary;
  #endif

    /*
      23. 表示进程亲属关系的成员
      1) real_parent: 指向其父进程，如果创建它的父进程不再存在，则指向 PID 为 1 的 init 进程
      2) parent: 指向其父进程，当它终止时，必须向它的父进程发送信号。它的值通常与 real_parent 相同
    ,*/
    struct task_struct *real_parent;
    struct task_struct *parent;
    /*
      3) children: 表示链表的头部，链表中的所有元素都是它的子进程(子进程链表)
      4) sibling: 用于把当前进程插入到兄弟链表中(连接到父进程的子进程链表(兄弟链表))
      5) group_leader: 指向其所在进程组的领头进程
    ,*/
    struct list_head children;
    struct list_head sibling;
    struct task_struct *group_leader;

    struct list_head ptraced;
    struct list_head ptrace_entry;
    struct bts_context *bts;

    /*
      24. pids
      PID 散列表和链表
    ,*/
    struct pid_link pids[PIDTYPE_MAX];
    /*
      25. thread_group
      线程组中所有进程的链表
    ,*/
    struct list_head thread_group;

    /*
      26. do_fork 函数
      1) vfork_done
      在执行 do_fork()时，如果给定特别标志，则 vfork_done 会指向一个特殊地址
      2) set_child_tid、clear_child_tid
      如果 copy_process 函数的 clone_flags 参数的值被置为 CLONE_CHILD_SETTID 或 CLONE_CHILD_CLEARTID，则会把 child_tidptr 参数的值分别复制到 set_child_tid 和 clear_child_tid 成员。这些标志说明必须改变子
      进程用户态地址空间的 child_tidptr 所指向的变量的值。
    ,*/
    struct completion *vfork_done;
    int __user *set_child_tid;
    int __user *clear_child_tid;

    /*
      27. 记录进程的 I/O 计数(时间)
      1) utime
      用于记录进程在"用户态"下所经过的节拍数(定时器)
      2) stime
      用于记录进程在"内核态"下所经过的节拍数(定时器)
      3) utimescaled
      用于记录进程在"用户态"的运行时间，但它们以处理器的频率为刻度
      4) stimescaled
      用于记录进程在"内核态"的运行时间，但它们以处理器的频率为刻度
    ,*/
    cputime_t utime, stime, utimescaled, stimescaled;
    /*
      5) gtime
      以节拍计数的虚拟机运行时间(guest time)
    ,*/
    cputime_t gtime;
    /*
      6) prev_utime、prev_stime 是先前的运行时间
    ,*/
    cputime_t prev_utime, prev_stime;
    /*
      7) nvcsw
      自愿(voluntary)上下文切换计数
      8) nivcsw
      非自愿(involuntary)上下文切换计数
    ,*/
    unsigned long nvcsw, nivcsw;
    /*
      9) start_time
      进程创建时间
      10) real_start_time
      进程睡眠时间，还包含了进程睡眠时间，常用于/proc/pid/stat，
    ,*/
    struct timespec start_time;
    struct timespec real_start_time;
    /*
      11) cputime_expires
      用来统计进程或进程组被跟踪的处理器时间，其中的三个成员对应着 cpu_timers[3]的三个链表
    ,*/
    struct task_cputime cputime_expires;
    struct list_head cpu_timers[3];
  #ifdef CONFIG_DETECT_HUNG_TASK
    /*
      12) last_switch_count
      nvcsw 和 nivcsw 的总和
    ,*/
    unsigned long last_switch_count;
  #endif
    struct task_io_accounting ioac;
  #if defined(CONFIG_TASK_XACCT)
    u64 acct_rss_mem1;
    u64 acct_vm_mem1;
    cputime_t acct_timexpd;
  #endif

    /*
      28. 缺页统计
    ,*/
    unsigned long min_flt, maj_flt;

    /*
      29. 进程权能
    ,*/
    const struct cred *real_cred;
    const struct cred *cred;
    struct mutex cred_guard_mutex;
    struct cred *replacement_session_keyring;

    /*
      30. comm[TASK_COMM_LEN]
      相应的程序名
    ,*/
    char comm[TASK_COMM_LEN];

    /*
      31. 文件
      1) fs
      用来表示进程与文件系统的联系，包括当前目录和根目录
      2) files
      表示进程当前打开的文件
    ,*/
    int link_count, total_link_count;
    struct fs_struct *fs;
    struct files_struct *files;

  #ifdef CONFIG_SYSVIPC
    /*
      32. sysvsem
      进程通信(SYSVIPC)
    ,*/
    struct sysv_sem sysvsem;
  #endif

    /*
      33. 处理器特有数据
    ,*/
    struct thread_struct thread;

    /*
      34. nsproxy
      命名空间
    ,*/
    struct nsproxy *nsproxy;

    /*
      35. 信号处理
      1) signal: 指向进程的信号描述符
      2) sighand: 指向进程的信号处理程序描述符
    ,*/
    struct signal_struct *signal;
    struct sighand_struct *sighand;
    /*
      3) blocked: 表示被阻塞信号的掩码
      4) real_blocked: 表示临时掩码
    ,*/
    sigset_t blocked, real_blocked;
    sigset_t saved_sigmask;
    /*
      5) pending: 存放私有挂起信号的数据结构
    ,*/
    struct sigpending pending;
    /*
      6) sas_ss_sp: 信号处理程序备用堆栈的地址
      7) sas_ss_size: 表示堆栈的大小
    ,*/
    unsigned long sas_ss_sp;
    size_t sas_ss_size;
    /*
      8) notifier
      设备驱动程序常用 notifier 指向的函数来阻塞进程的某些信号
      9) otifier_data
      指的是 notifier 所指向的函数可能使用的数据。
      10) otifier_mask
      标识这些信号的位掩码
    ,*/
    int (*notifier)(void *priv);
    void *notifier_data;
    sigset_t *notifier_mask;

    /*
      36. 进程审计
    ,*/
    struct audit_context *audit_context;
  #ifdef CONFIG_AUDITSYSCALL
    uid_t loginuid;
    unsigned int sessionid;
  #endif

    /*
      37. secure computing
    ,*/
    seccomp_t seccomp;

    /*
      38. 用于 copy_process 函数使用 CLONE_PARENT 标记时
    ,*/
    u32 parent_exec_id;
    u32 self_exec_id;

    /*
      39. alloc_lock
      用于保护资源分配或释放的自旋锁
    ,*/
    spinlock_t alloc_lock;

    /*
      40. 中断
    ,*/
  #ifdef CONFIG_GENERIC_HARDIRQS
    struct irqaction *irqaction;
  #endif
  #ifdef CONFIG_TRACE_IRQFLAGS
    unsigned int irq_events;
    int hardirqs_enabled;
    unsigned long hardirq_enable_ip;
    unsigned int hardirq_enable_event;
    unsigned long hardirq_disable_ip;
    unsigned int hardirq_disable_event;
    int softirqs_enabled;
    unsigned long softirq_disable_ip;
    unsigned int softirq_disable_event;
    unsigned long softirq_enable_ip;
    unsigned int softirq_enable_event;
    int hardirq_context;
    int softirq_context;
  #endif

    /*
      41. pi_lock
      task_rq_lock 函数所使用的锁
    ,*/
    spinlock_t pi_lock;

  #ifdef CONFIG_RT_MUTEXES
    /*
      42. 基于 PI 协议的等待互斥锁，其中 PI 指的是 priority inheritance/9 优先级继承)
    ,*/
    struct plist_head pi_waiters;
    struct rt_mutex_waiter *pi_blocked_on;
  #endif

  #ifdef CONFIG_DEBUG_MUTEXES
    /*
      43. blocked_on
      死锁检测
    ,*/
    struct mutex_waiter *blocked_on;
  #endif

    /*
      44. lockdep，
    ,*/
  #ifdef CONFIG_LOCKDEP
  # define MAX_LOCK_DEPTH 48UL
    u64 curr_chain_key;
    int lockdep_depth;
    unsigned int lockdep_recursion;
    struct held_lock held_locks[MAX_LOCK_DEPTH];
    gfp_t lockdep_reclaim_gfp;
  #endif

    /*
      45. journal_info
      JFS 文件系统
    ,*/
    void *journal_info;

    /*
      46. 块设备链表
    ,*/
    struct bio *bio_list, **bio_tail;

    /*
      47. reclaim_state
      内存回收
    ,*/
    struct reclaim_state *reclaim_state;

    /*
      48. backing_dev_info
      存放块设备 I/O 数据流量信息
    ,*/
    struct backing_dev_info *backing_dev_info;

    /*
      49. io_context
      I/O 调度器所使用的信息
    ,*/
    struct io_context *io_context;

    /*
      50. CPUSET 功能
    ,*/
  #ifdef CONFIG_CPUSETS
    nodemask_t mems_allowed;
    int cpuset_mem_spread_rotor;
  #endif

    /*
      51. Control Groups
    ,*/
  #ifdef CONFIG_CGROUPS
    struct css_set *cgroups;
    struct list_head cg_list;
  #endif

    /*
      52. robust_list
      futex 同步机制
    ,*/
  #ifdef CONFIG_FUTEX
    struct robust_list_head __user *robust_list;
  #ifdef CONFIG_COMPAT
    struct compat_robust_list_head __user *compat_robust_list;
  #endif
    struct list_head pi_state_list;
    struct futex_pi_state *pi_state_cache;
  #endif
  #ifdef CONFIG_PERF_EVENTS
    struct perf_event_context *perf_event_ctxp;
    struct mutex perf_event_mutex;
    struct list_head perf_event_list;
  #endif

    /*
      53. 非一致内存访问(NUMA  Non-Uniform Memory Access)
    ,*/
  #ifdef CONFIG_NUMA
    struct mempolicy *mempolicy;    /* Protected by alloc_lock */
    short il_next;
  #endif

    /*
      54. fs_excl
      文件系统互斥资源
    ,*/
    atomic_t fs_excl;

    /*
      55. rcu
      RCU 链表
    ,*/
    struct rcu_head rcu;

    /*
      56. splice_pipe
      管道
    ,*/
    struct pipe_inode_info *splice_pipe;

    /*
      57. delays
      延迟计数
    ,*/
  #ifdef    CONFIG_TASK_DELAY_ACCT
    struct task_delay_info *delays;
  #endif

    /*
      58. make_it_fail
      fault injection
    ,*/
  #ifdef CONFIG_FAULT_INJECTION
    int make_it_fail;
  #endif

    /*
      59. dirties
      FLoating proportions
    ,*/
    struct prop_local_single dirties;

    /*
      60. Infrastructure for displayinglatency
    ,*/
  #ifdef CONFIG_LATENCYTOP
    int latency_record_count;
    struct latency_record latency_record[LT_SAVECOUNT];
  #endif

    /*
      61. time slack values，常用于 poll 和 select 函数
    ,*/
    unsigned long timer_slack_ns;
    unsigned long default_timer_slack_ns;

    /*
      62. scm_work_list
      socket 控制消息(control message)
    ,*/
    struct list_head    *scm_work_list;

    /*
      63. ftrace 跟踪器
    ,*/
  #ifdef CONFIG_FUNCTION_GRAPH_TRACER
    int curr_ret_stack;
    struct ftrace_ret_stack    *ret_stack;
    unsigned long long ftrace_timestamp;
    atomic_t trace_overrun;
    atomic_t tracing_graph_pause;
  #endif
  #ifdef CONFIG_TRACING
    unsigned long trace;
    unsigned long trace_recursion;
  #endif
  };
#+END_SRC


* Select, Poll, Epoll 区别

每次调用 select 和 poll 都需要把文件描述符集合复制到内核当中, 可能是潜在
几百 KB 的开销, 同时返回时候会线性搜索检测的文件描述符

Epoll 创建时候会在内核分配一颗红黑树来保存检测的文件描述符, 因此没有调用开销.
同时在内核中断里为每一个文件描述符注册了一个回调函数, 把就绪的文件描述符加入一个
ready list 当中.

* 历史
  Multis -> Unics(汇编) -> Unix(C) -> BSD(Unix 分支)/System V(Official)
  -> Minix(Mini unix,Unix-like) -> Linux


* 进程
  内核把进程存放在一个双向循环链表, 称作任务队列(task list).

  2.6 以后, task_struct 不再直接放入内核栈, 而是在内核栈的栈底或者栈顶, 创建一个新的结构 thread_info.

#+BEGIN_SRC c
  struct thread_info {
    struct pcb_struct pcb;    /* palcode state */

    struct task_struct  *task;    /* main task structure */
    unsigned int    flags;    /* low level flags */
    unsigned int    ieee_state; /* see fpu.h */

    struct exec_domain  *exec_domain; /* execution domain */
    mm_segment_t    addr_limit; /* thread address space */
    unsigned    cpu;    /* current CPU */
    int     preempt_count; /* 0 => preemptable, <0 => BUG */

    int bpt_nsaved;
    unsigned long bpt_addr[2];    /* breakpoint handling  */
    unsigned int bpt_insn[2];

    struct restart_block  restart_block;
  };

#+END_SRC

其中的 task 指向该任务实际 task_struct 的指针.

内核查看 task_struct 过程中, 先要调用 current 宏得到线程内核栈, 然后调用 current_thread_info()
计算 thread_info 的偏移,最后从 thread_info 中的 task 得到 task_struct.

** 进程状态
   + TASK_RUNNING: 进程正在执行或者在 run_queue 中等待执行. 也是进程在用户空间中执行的唯一可能状态.
   + TASK_INTERRUPTIBLE: 进程正在睡眠(阻塞), 位于某个 wait_queue. 一旦睡眠条件解除, 内核会把进程设置为 TASK_RUNNING.
     处于此状态的进程也会因为接收到信号而提前被唤醒并随时准备投入运行.
   + TASK_UNINTERRUPTIBLE: 同上, 但处于此状态的任务对信号不做响应, 只能由内核唤醒, 例如 wake_up().
   + TASK_ZOMBIE: 进程已经终止, 但父进程还没有执行 wait()系统调用, 终止进程的信息也还没有回收.
   + TASK_STOPPED: 进程此时处于暂停状态来接受某个特殊处理. 通常进程接收到 SIGSTOP, SIGTSTP, SIGTTIN 或 SIGTTOU
     信号后就处于这种状态. 例如正接受调试的进程.
   + TASK_SWAPPING: 换入/换出

*** 设置当前进程状态.
     内核使用 set_task_state(task, state)来调整某个进程的状态.

     
** 进程创建  
   Unix 的进程创建分为两个阶段, fork()和 exec().
   
   fork()复制当前进程来创建一个子进程, 两者区别仅在于 PID, PPID, 和某些资源和统计量(例如, 挂起的信号).
   exec()复制读取可执行文件并将其载入地址空间开始运行.

*** 写时拷贝(copy-on-write)
    fork()并不复制整个今晨地址空间,而是让父子进程共享同一个拷贝, 因此实际开销就是复制父进程的页表以及给子进程
    创建唯一的进程描述符.

    资源的复制只有在需要写入的时候才进行,在此之前,只是以只读方式共享.
    
*** fork()
    linux 通过 clone()系统调用实现 fork(). 这个调用通过一系列的参数标志来指名父子进程需要共享的资源.

    然后 clone()去调用 do_fork(), do_fork()完成了创建的大部分工作, 该函数调用 copy_process(), 然后让
    进程开始运行.

    copy_process()完成了这些工作:
    + 调用 dup_task_struct()为新进程创建一个内核栈, thread_info 结构和 task_struct 结构, 这些值和父进程
      一模一样, 此时子进程和父进程的描述符是完全相同的.
    + 检查并确保新创建这个子进程后, 当前用户所拥有的进程数目没有超过给它分配的资源的限制.
    + 子进程着手是自己和父进程区别开来, 进程描述符许多成员都要被清零或设为初始值. task_struct 的大多数数据
      未被修改.
    + 子进程状态设为 TASK_UNINTERRUPTIBLE, 确保它不会运行.
    + copy_process()调用 copy_flags()以更新 task_struct 的 flags 成员.
    + 调用 alloc_pid()为新进程分配一个有效的 PID.
    + 根据传递给 clone()的参数标志, copy_process()拷贝或共享打开的文件, 文件系统信息, 信号处理函数, 进程地址空间
      和命名空间. 一般情况下, 这些资源会给父进程的所有子线程共享.
    + 最后, copy_process()做扫尾工作并返回一个指向子进程的指针.


** 创建线程
   线程的创建和普通进程的创建类似, 只不过在调用 clone()时候需要传递一些参数标志来指明需要共享的资源.

   clone(CLONE_VM | CLONE_FS | CLONE_FILES | CONE_SIGHAND, 0).

   上面的代码结果和调用 fork()差不多, 只是父子共享地址空间, 文件系统资源, 文件描述符和信号处理程序.

** 进程终结
   进程通过调用系统调用 exit()来终结, 终结的大部分靠 do_exit()完成.

   do_exit()完成以下工作.

   1. 把 task_struct 的 flags 设置为 PF_EXITING
   2. 调用 del_timer_sync()删除任一内核定时器.
   3. 调用 exit_mm()函数释放进程占用的 mm_struct, 如果没有别的进程共享它们, 就彻底释放它们.
   4. 接下来调用 sem_exit()函数, 如果进程排队等候 IPC 信号, 它则离开队列.
   5. 调用 exit_files()和 exit_fs(), 分别递减文件描述符, 文件系统数据的引用计数, 如果某个引用计数
      降为 0, 代表没有进程在使用相应资源, 释放对应资源.
   6. 接着把放在 task_struct 的 exit_code 成员中的任务退出代码置为由 exit()提供的退出代码,
      或者去完成任何内核机制规定的退出动作.
   7. 调用 exit_notify()向父进程发送信号, 给子进程寻找养父, 养父为线程组的其他线程或者为 init 进程.
      并把 exit_state 设为 EXIT_ZOMBIE.
   8. do_exit()调用 schedule()切换到新的进程. 因为处于 EXIT_ZOMBIE 的进程不会被调度, 所以
      这是进程执行的最后一段代码. do_exit()永不返回.

   至此, 与进程相关的资源都被释放掉了(假设该进程是唯一使用者), 进程不可运行(实际也没有地址空间给它运行)并处于
   EXIT_ZOMBIE 退出状态. 它占用的所有内存就是内核栈, thread_info 结构和 task_struct 结构. 此时进程存在的
   唯一目的就是向它的父进程提供信息. 父进程检索到信息后, 或者通知内核那是无关的信息后, 由进程持有的剩余内存被释放,
   归还给系统使用.

   
*** 删除进程描述符
    父进程通过 wait()系统调用收集其后代的信息.

    wait()函数族: 挂起调用它的进程, 直到其中的一个子进程退出, 此时函数会返回该子进程的 PID. 此外,调用该函数时提供的指针会包含子函数退出时
    的退出代码.

  
*** 孤儿进程
    如果父进程在子进程之前退出了, 必须有机制来保证子进程能找到一个新的父亲, 否则成为孤儿的进程永远处于僵死状态, 白白耗费内存.
    
    解决方法是给子进程在当前线程组内找一个线程组内作为父亲, 如果找不到, 就让 init 作为他们的父进程.

* 进程调度 
** 进程优先级
   Linux 采用了两种不同的优先级范围.

   第一种是用 nice 值, [-20, 19], 默认 0. nice 越高, 获得的 CPU 时间越少. 不同的 Unix 系统对 nice 运用方式有所不同, Mac OS X, 进程的 nice
   决定分配给进程时间片的绝对数值, linux 中, nice 代表时间片的比例.

   第二种是实时优先级, 默认变化范围[0, 99]. 与 nice 相反, 实时优先级数值越高, 进程优先级越高. 任何实时进程的优先级都高于普通进程.
   也就是实时优先级和 nice 优先级处于互不相交的两个范畴.

   Linux 的 CFS 调度器并没有直接分配时间片给进程, 它将处理器的使用比例划分给了进程. 这种情况下, 进程所获得的处理器时间其实是和系统负载密切相关.

   这个比例进一步还会收进程 nice 值的影响, nice 作为权重将调整进程所使用的 CPU 时间比. 具有高 nice 值的进程会被赋予低权重, 低 nice 值的进程会被赋予高权重.

   Linux 使用 CFS 调度器, 进程抢占时机取决于新的可运行进程消耗了多少处理器时间比, 如果消耗的使用比例比当前进程小, 则新进程立刻投入运行, 抢占当前进程.

** 调度器类   
   Linux 调度器是以模块方式提供的, 目的是运行不同类型的进程可以有针对性地选择调度算法.

   这种模块化结构被称为调度器类(scheduler classes), 它允许多种不同的可动态添加的调度算法并存, 调度属于自己范畴的进程.

   每个调度器有一个优先级, 内核会按照优先级顺序遍历调度器, 拥有一个可执行进程的最高优先级的调度器类胜出, 选择执行那一个线程.

   完全公平调度(CFS)是一个针对普通进程的调度类, linux 中称为 SCHED_NORMAL.
   
** 公平调度
   CFS 出发点基于一个简单的理念: 每个进程将能获得 1/n 的处理器时间, n 指可运行进程的数量.

   CFS 在所有可运行进程总数基础上计算出某个进程应该运行多久, 而不是依靠 nice 值计算时间片. nice 值在 CFS 中用作进程获得处理器运行比的权重.

   CFS 使用 targeted latency 作为调度周期来计算 timeslice, latency 越小, 越接近完美多任务.

   CFS 使用 minimum granularity 来表示 timeslice 的最小值, 这个值越小, 会加重 CPU 切换进程的负载.

   总结: 任何进程获得的 CPU 时间是由它自己和其他所有可运行进程 nice 值的相对差值决定的. nice 值对时间片的作用不再是算数加权, 而是几何加权.

   
** 时间记账
   所有调度器都必须对进程运行时间做记账.
   
   CFS 使用调度器实体结构来追踪进程运行记账:
   
#+BEGIN_SRC c
  struct sched_entity {
    struct load_weight  load;   /* for load-balancing */
    struct rb_node    run_node;
    struct list_head  group_node;
    unsigned int    on_rq;

    u64     exec_start;
    u64     sum_exec_runtime;
    u64     vruntime;
    u64     prev_sum_exec_runtime;

    u64     nr_migrations;

  #ifdef CONFIG_SCHEDSTATS
    struct sched_statistics statistics;
  #endif

  #ifdef CONFIG_FAIR_GROUP_SCHED
    struct sched_entity *parent;
    /* rq on which this entity is (to be) queued: */
    struct cfs_rq   *cfs_rq;
    /* rq "owned" by this entity/group: */
    struct cfs_rq   *my_q;
  #endif
  };
#+END_SRC

vruntime 变量存放进程的虚拟运行时间, 该运行时间的计算是经过了所有可运行进程总数的标准化. 虚拟时间以 ns 为单位,
所以 vruntime 和定时器节拍不再相关.

CFS 使用了 vruntime 变量来记录一个程序到底运行了多长时间以及它应该再运行多久.

#+BEGIN_SRC c
  static void update_curr(struct cfs_rq *cfs_rq)
  {
    struct sched_entity *curr = cfs_rq->curr;
    u64 now = rq_of(cfs_rq)->clock_task;
    unsigned long delta_exec;

    if (unlikely(!curr))
      return;

    /*
     ,* Get the amount of time the current task was running
     ,* since the last time we changed load (this cannot
     ,* overflow on 32 bits):
     ,*/
    delta_exec = (unsigned long)(now - curr->exec_start);
    if (!delta_exec)
      return;

    __update_curr(cfs_rq, curr, delta_exec);
    curr->exec_start = now;

    if (entity_is_task(curr)) {
      struct task_struct *curtask = task_of(curr);

      trace_sched_stat_runtime(curtask, delta_exec, curr->vruntime);
      cpuacct_charge(curtask, delta_exec);
      account_group_exec_runtime(curtask, delta_exec);
    }
  }
#+END_SRC

update_curr()计算了当前进程的执行时间, 并且将其存放在变量 delta_exec 中. 然后它又将运行时间传递给了__update_curr(), 
又后者再根据当前可运行进程总数对运行时间进行加权计算, 最终将权重值与当前运行进程的 vruntime 相加.

#+BEGIN_SRC c
  /*
   ,* Update the current task's runtime statistics. Skip current tasks that
   ,* are not in our scheduling class.
   ,*/
  static inline void
  __update_curr(struct cfs_rq *cfs_rq, struct sched_entity *curr,
                unsigned long delta_exec)
  {
    unsigned long delta_exec_weighted;

    schedstat_set(curr->statistics.exec_max,
                  max((u64)delta_exec, curr->statistics.exec_max));

    curr->sum_exec_runtime += delta_exec;
    schedstat_add(cfs_rq, exec_clock, delta_exec);
    delta_exec_weighted = calc_delta_fair(delta_exec, curr);

    curr->vruntime += delta_exec_weighted;
    update_min_vruntime(cfs_rq);

  #if defined CONFIG_SMP && defined CONFIG_FAIR_GROUP_SCHED
    cfs_rq->load_unacc_exec_time += delta_exec;
  #endif
  }
#+END_SRC


update_curr()是有系统定时器周期性调用的, 因此无论进程处于可运行, 还是被堵塞, vruntime 可以准确的测量给定进程的运行时间,
而且可知道谁应该是下一个被运行的进程.

** 进程选择
   CFS 调度算法的核心, 选择具有最小的 vruntime 的 task.

   CFS 使用红黑树来组织可运行进程队列, 并利用其迅速找到最小 vruntime 值的进程.

   在 Linux 值里面, 红黑树被称为 rbtree, 自平衡二叉树.

*** 挑选下一个任务
    假设有一个红黑树存储了系统中所有可运行进程. 其节点的键值便是可运行进程的 vruntime.
    
    CFS 调度器选取待运行的下一个进程, 是所有进程中 vruntime 最小的那个, 它对应的是树中最左侧的叶子节点.

    CFS 的进程选择算法可简单总结在"运行 rbtree 树中最左边叶子节点代表的进程".

    实现这一个过程的函数是__pick_next_entity()

    #+BEGIN_SRC c
      static struct sched_entity *__pick_first_entity(struct cfs_rq *cfs_rq)
      {
        struct rb_node *left = cfs_rq->rb_leftmost;

        if (!left)
          return NULL;

        return rb_entry(left, struct sched_entity, run_node);
      }

      static struct sched_entity *__pick_next_entity(struct sched_entity *se)
      {
        struct rb_node *next = rb_next(&se->run_node);

        if (!next)
          return NULL;

        return rb_entry(next, struct sched_entity, run_node);
      }
    #+END_SRC

    实际上 linux 把最左叶子点点缓存在一个 rb_leftmost 字段中, 因此可直接调用.


*** 向树中加入进程
    CFS 把进程加入 rbtree 中, 以及缓存最左叶子节点的过程发生在进程变为可运行状态(被唤醒), 或者是通过 fork()调用第一次创建进程时候.
    enqueue_entity()实现了这个过程.

    #+BEGIN_SRC c
      static void
      enqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)
      {
        /*
         ,* Update the normalized vruntime before updating min_vruntime
         ,* through callig update_curr().
         ,*/
        if (!(flags & ENQUEUE_WAKEUP) || (flags & ENQUEUE_WAKING))
          se->vruntime += cfs_rq->min_vruntime;

        /*
         ,* Update run-time statistics of the 'current'.
         ,*/
        update_curr(cfs_rq);
        update_cfs_load(cfs_rq, 0);
        account_entity_enqueue(cfs_rq, se);
        update_cfs_shares(cfs_rq);

        if (flags & ENQUEUE_WAKEUP) {
          place_entity(cfs_rq, se, 0);
          enqueue_sleeper(cfs_rq, se);
        }

        update_stats_enqueue(cfs_rq, se);
        check_spread(cfs_rq, se);
        if (se != cfs_rq->curr)
          __enqueue_entity(cfs_rq, se);
        se->on_rq = 1;

        if (cfs_rq->nr_running == 1)
          list_add_leaf_cfs_rq(cfs_rq);
      }
    #+END_SRC

    该函数更新运行时间和其他一些统计数据, 然后调用__enqueue_entity()进行插入操作.

    #+BEGIN_SRC c
      /*
       ,* Enqueue an entity into the rb-tree:
       ,*/
      static void __enqueue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)
      {
        struct rb_node **link = &cfs_rq->tasks_timeline.rb_node;
        struct rb_node *parent = NULL;
        struct sched_entity *entry;
        s64 key = entity_key(cfs_rq, se);
        int leftmost = 1;

        /*
         ,* Find the right place in the rbtree:
         ,*/
        while (*link) {
          parent = *link;
          entry = rb_entry(parent, struct sched_entity, run_node);
          /*
           ,* We dont care about collisions. Nodes with
           ,* the same key stay together.
           ,*/
          if (key < entity_key(cfs_rq, entry)) {
            link = &parent->rb_left;
          } else {
            link = &parent->rb_right;
            leftmost = 0;
          }
        }

        /*
         ,* Maintain a cache of leftmost tree entries (it is frequently
         ,* used):
         ,*/
        if (leftmost)
          cfs_rq->rb_leftmost = &se->run_node;

        rb_link_node(&se->run_node, parent, link);
        rb_insert_color(&se->run_node, &cfs_rq->tasks_timeline);
      }
    #+END_SRC


*** 从树中删除进程
    从 rbtree 中删除进程, 发生在进程堵塞或者终止时.

    #+BEGIN_SRC c
      static void
      dequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se, int flags)
      {
        /*
         ,* Update run-time statistics of the 'current'.
         ,*/
        update_curr(cfs_rq);

        update_stats_dequeue(cfs_rq, se);
        if (flags & DEQUEUE_SLEEP) {
      #ifdef CONFIG_SCHEDSTATS
          if (entity_is_task(se)) {
            struct task_struct *tsk = task_of(se);

            if (tsk->state & TASK_INTERRUPTIBLE)
              se->statistics.sleep_start = rq_of(cfs_rq)->clock;
            if (tsk->state & TASK_UNINTERRUPTIBLE)
              se->statistics.block_start = rq_of(cfs_rq)->clock;
          }
      #endif
        }

        clear_buddies(cfs_rq, se);

        if (se != cfs_rq->curr)
          __dequeue_entity(cfs_rq, se);
        se->on_rq = 0;
        update_cfs_load(cfs_rq, 0);
        account_entity_dequeue(cfs_rq, se);
        update_min_vruntime(cfs_rq);
        update_cfs_shares(cfs_rq);

        /*
         ,* Normalize the entity after updating the min_vruntime because the
         ,* update can refer to the ->curr item and we need to reflect this
         ,* movement in our normalized position.
         ,*/
        if (!(flags & DEQUEUE_SLEEP))
          se->vruntime -= cfs_rq->min_vruntime;
      }
    #+END_SRC
    和红黑树添加进程一样, 实际工作是由辅助函数__dequeue_entity()完成.

    #+BEGIN_SRC c
      static void __dequeue_entity(struct cfs_rq *cfs_rq, struct sched_entity *se)
      {
        if (cfs_rq->rb_leftmost == &se->run_node) {
          struct rb_node *next_node;

          next_node = rb_next(&se->run_node);
          cfs_rq->rb_leftmost = next_node;
        }

        rb_erase(&se->run_node, &cfs_rq->tasks_timeline);
      }
    #+END_SRC

** 调度器入口
* 磁盘

  1. 寻道时间
  2. 旋转时间
     
     磁盘的读写基本单位是扇区, 是一个物理概念, 是 512 字节.

     操作系统的文件系统操作文件的最小单位是块, 是一个抽象概念.
     操作系统与内存打交道, 是以页的概念作为最小单位.
     
  磁盘顺序读取效率很高, 因为没有寻道时间, 只需要很少的旋转时间. 因此对于具有局部性的程序来说, 预读可以提高 I/O 效率.

  预读的长度一般为页的整倍数.

  linux 要求块的大小是 2 的 n 次方乘以扇区的大小, 内存页的大小大于等于块的大小, 大多数时候页大小等于块大小, 都是 4K, 即是 8 个扇区大小.

** 局部性原理
   当一个数据被用到时, 其附近的数据通常也会马上被使用.

* 数据库
  数据库设计者巧妙利用了磁盘预读原理, 将一个节点的大小设为一个页的大小, 这样每个节点只需要一次 I/O 就可以完全载入.

  B-Tree 一次检索最多需要 h-1 次 I/O(根节点常驻内存), 复杂度为 O(h)=O(logdN). 一般实际应用中, 度 d 是个非常大的数字,
  通常超过 100, 因此 h 非常小(通常不超过 3).

  因此, 用 B-Tree 作为索引结构的效率非常高.

  红黑树这种结构, h 明显深得多, 效率明显比 B-Tree 差得多.

** 存储引擎
   在 MySQL 中, 索引属于存储引擎级别的概念, 不同存储引擎对索引的实现方式是不同的.

*** MyISAM 索引实现
    每个叶节点的 data 域保存的是数据记录的地址.
    索引解锁的算法为首先按照 B+Tree 搜索算法搜索索引, 如果指定 Key 存在, 则取出其 data 域的值, 然后以 data 域的值为地址, 读取相应数据记录.

    MyISAM 这种索引方式也叫作非聚簇的, 这么称呼是为了与 InnoDB 的聚簇索引区分.
    
*** InnoDB 索引实现
    第一个重大区别是 InnoDB 的数据文件本身就是索引文件, 上文讨论的 MyISAM 索引文件和数据文件是分离的, 索引文件仅仅保存数据记录的地址.
    
    而在 InnoDB 中, 表数据文件本身就是按 B+Tree 组织的一个索引结构, 这棵树的叶节点 data 域保存了完整的数据记录. 因此 InnoDB 的表数据就是主索引.

    InnoDB 的所有辅助索引都引用 Primary Key 作为 data 域.

    聚簇索引这种实现使得按主键的搜索十分高效, 但是辅助索引需要检索两遍索引: 首先检索辅助索引取得主键, 然后用主键到主索引中检索获得记录.

** 索引使用策略及优化
   MySQL 的优化主要分为结构优化和查询优化.

** 索引选择性与前缀索引
   索引虽然加快了查询速度, 但索引文件本身需要消耗存储空间, 同时索引会加大插入,删除和修改记录的负担, 另外 MySQL 运行时候也要消耗资源维持索引, 因此索引不是越多越好.

** InnoDB 的主键选择与插入优化.
   在使用 InnoDB 存储引擎时, 如果没有特别的需要, 请使用一个与业务无关的自增字段作为主键.

   主要与 B+Tree 的插入操作有关, 如果使用自增主键, 每次插入新纪录, 记录会顺序添加到索引的最后位置.
   如果使用非自增主键, 由于每次主键的值近乎随机, 每次新纪录都会插入到现有的索引页的某个中间位置, MySQL 不得不为了将新纪录插到合适位置而移动数据, 缓存也要更新, 这增加了很多开销.
